{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOyNX3HaZl2nY6JlUek/hOk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/eoeelocr/AI_Project/blob/main/Torchvision_object_detection_finetuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install --upgrade torch torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0O3wt_yvsveg",
        "outputId": "27b33cb0-bbaa-46a7-f0b1-332b320b82d6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.4)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch) (12.1.105)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch) (12.2.140)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.7.22)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "xGgQg4kOr8Lb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "from torchvision.io import read_image\n",
        "from torchvision.ops.boxes import masks_to_boxes\n",
        "from torchvision import tv_tensors\n",
        "from torchvision.transforms.v2 import functional as F\n",
        "\n",
        "\n",
        "class PennFudanDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root, transforms):\n",
        "        self.root = root\n",
        "        self.transforms = transforms\n",
        "        # load all image files, sorting them to\n",
        "        # ensure that they are aligned\n",
        "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
        "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # load images and masks\n",
        "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
        "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
        "        img = read_image(img_path)\n",
        "        mask = read_image(mask_path)\n",
        "        # instances are encoded as different colors\n",
        "        obj_ids = torch.unique(mask)\n",
        "        # first id is the background, so remove it\n",
        "        obj_ids = obj_ids[1:]\n",
        "        num_objs = len(obj_ids)\n",
        "\n",
        "        # split the color-encoded mask into a set\n",
        "        # of binary masks\n",
        "        masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8)\n",
        "\n",
        "        # get bounding box coordinates for each mask\n",
        "        boxes = masks_to_boxes(masks)\n",
        "\n",
        "        # there is only one class\n",
        "        labels = torch.ones((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        image_id = idx\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "        # suppose all instances are not crowd\n",
        "        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n",
        "\n",
        "        # Wrap sample and targets into torchvision tv_tensors:\n",
        "        img = tv_tensors.Image(img)\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = tv_tensors.BoundingBoxes(boxes, format=\"XYXY\", canvas_size=F.get_size(img))\n",
        "        target[\"masks\"] = tv_tensors.Mask(masks)\n",
        "        target[\"labels\"] = labels\n",
        "        target[\"image_id\"] = image_id\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        if self.transforms is not None:\n",
        "            img, target = self.transforms(img, target)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.imgs)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "\n",
        "# load a model pre-trained on COCO\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "\n",
        "# replace the classifier with a new one, that has\n",
        "# num_classes which is user-defined\n",
        "num_classes = 2  # 1 class (person) + background\n",
        "# get number of input features for the classifier\n",
        "in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "# replace the pre-trained head with a new one\n",
        "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)"
      ],
      "metadata": {
        "id": "YCNUYAQ_t_Uo"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection import FasterRCNN\n",
        "from torchvision.models.detection.rpn import AnchorGenerator\n",
        "\n",
        "# load a pre-trained model for classification and return\n",
        "# only the features\n",
        "backbone = torchvision.models.mobilenet_v2(weights=\"DEFAULT\").features\n",
        "# ``FasterRCNN`` needs to know the number of\n",
        "# output channels in a backbone. For mobilenet_v2, it's 1280\n",
        "# so we need to add it here\n",
        "backbone.out_channels = 1280\n",
        "\n",
        "# let's make the RPN generate 5 x 3 anchors per spatial\n",
        "# location, with 5 different sizes and 3 different aspect\n",
        "# ratios. We have a Tuple[Tuple[int]] because each feature\n",
        "# map could potentially have different sizes and\n",
        "# aspect ratios\n",
        "anchor_generator = AnchorGenerator(\n",
        "    sizes=((32, 64, 128, 256, 512),),\n",
        "    aspect_ratios=((0.5, 1.0, 2.0),)\n",
        ")\n",
        "\n",
        "# let's define what are the feature maps that we will\n",
        "# use to perform the region of interest cropping, as well as\n",
        "# the size of the crop after rescaling.\n",
        "# if your backbone returns a Tensor, featmap_names is expected to\n",
        "# be [0]. More generally, the backbone should return an\n",
        "# ``OrderedDict[Tensor]``, and in ``featmap_names`` you can choose which\n",
        "# feature maps to use.\n",
        "roi_pooler = torchvision.ops.MultiScaleRoIAlign(\n",
        "    featmap_names=['0'],\n",
        "    output_size=7,\n",
        "    sampling_ratio=2,\n",
        ")\n",
        "\n",
        "# put the pieces together inside a Faster-RCNN model\n",
        "model = FasterRCNN(\n",
        "    backbone,\n",
        "    num_classes=2,\n",
        "    rpn_anchor_generator=anchor_generator,\n",
        "    box_roi_pool=roi_pooler,\n",
        ")"
      ],
      "metadata": {
        "id": "z2H_3fGQuDkd"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "\n",
        "\n",
        "def get_model_instance_segmentation(num_classes):\n",
        "    # load an instance segmentation model pre-trained on COCO\n",
        "    model = torchvision.models.detection.maskrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "\n",
        "    # get number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "    # replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    # now get the number of input features for the mask classifier\n",
        "    in_features_mask = model.roi_heads.mask_predictor.conv5_mask.in_channels\n",
        "    hidden_layer = 256\n",
        "    # and replace the mask predictor with a new one\n",
        "    model.roi_heads.mask_predictor = MaskRCNNPredictor(\n",
        "        in_features_mask,\n",
        "        hidden_layer,\n",
        "        num_classes,\n",
        "    )\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "K7VrhV6JuDtT"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\")\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\")\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\")\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\")\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tftU_34huJsp",
        "outputId": "e233bef6-f7e4-4e2f-84f2-1d6d006b50a6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import sys\n",
        "import time\n",
        "\n",
        "import torch\n",
        "import torchvision.models.detection.mask_rcnn\n",
        "import utils\n",
        "from coco_eval import CocoEvaluator\n",
        "from coco_utils import get_coco_api_from_dataset\n",
        "\n",
        "\n",
        "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq, scaler=None):\n",
        "    model.train()\n",
        "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
        "    metric_logger.add_meter(\"lr\", utils.SmoothedValue(window_size=1, fmt=\"{value:.6f}\"))\n",
        "    header = f\"Epoch: [{epoch}]\"\n",
        "\n",
        "    lr_scheduler = None\n",
        "    if epoch == 0:\n",
        "        warmup_factor = 1.0 / 1000\n",
        "        warmup_iters = min(1000, len(data_loader) - 1)\n",
        "\n",
        "        lr_scheduler = torch.optim.lr_scheduler.LinearLR(\n",
        "            optimizer, start_factor=warmup_factor, total_iters=warmup_iters\n",
        "        )\n",
        "\n",
        "    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n",
        "        images = list(image.to(device) for image in images)\n",
        "        targets = [{k: v.to(device) if isinstance(v, torch.Tensor) else v for k, v in t.items()} for t in targets]\n",
        "        with torch.cuda.amp.autocast(enabled=scaler is not None):\n",
        "            loss_dict = model(images, targets)\n",
        "            losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "        # reduce losses over all GPUs for logging purposes\n",
        "        loss_dict_reduced = utils.reduce_dict(loss_dict)\n",
        "        losses_reduced = sum(loss for loss in loss_dict_reduced.values())\n",
        "\n",
        "        loss_value = losses_reduced.item()\n",
        "\n",
        "        if not math.isfinite(loss_value):\n",
        "            print(f\"Loss is {loss_value}, stopping training\")\n",
        "            print(loss_dict_reduced)\n",
        "            sys.exit(1)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        if scaler is not None:\n",
        "            scaler.scale(losses).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            losses.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        if lr_scheduler is not None:\n",
        "            lr_scheduler.step()\n",
        "\n",
        "        metric_logger.update(loss=losses_reduced, **loss_dict_reduced)\n",
        "        metric_logger.update(lr=optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "    return metric_logger\n",
        "\n",
        "\n",
        "def _get_iou_types(model):\n",
        "    model_without_ddp = model\n",
        "    if isinstance(model, torch.nn.parallel.DistributedDataParallel):\n",
        "        model_without_ddp = model.module\n",
        "    iou_types = [\"bbox\"]\n",
        "    if isinstance(model_without_ddp, torchvision.models.detection.MaskRCNN):\n",
        "        iou_types.append(\"segm\")\n",
        "    if isinstance(model_without_ddp, torchvision.models.detection.KeypointRCNN):\n",
        "        iou_types.append(\"keypoints\")\n",
        "    return iou_types\n",
        "\n",
        "\n",
        "@torch.inference_mode()\n",
        "def evaluate(model, data_loader, device):\n",
        "    n_threads = torch.get_num_threads()\n",
        "    # FIXME remove this and make paste_masks_in_image run on the GPU\n",
        "    torch.set_num_threads(1)\n",
        "    cpu_device = torch.device(\"cpu\")\n",
        "    model.eval()\n",
        "    metric_logger = utils.MetricLogger(delimiter=\"  \")\n",
        "    header = \"Test:\"\n",
        "\n",
        "    coco = get_coco_api_from_dataset(data_loader.dataset)\n",
        "    iou_types = _get_iou_types(model)\n",
        "    coco_evaluator = CocoEvaluator(coco, iou_types)\n",
        "\n",
        "    for images, targets in metric_logger.log_every(data_loader, 100, header):\n",
        "        images = list(img.to(device) for img in images)\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.synchronize()\n",
        "        model_time = time.time()\n",
        "        outputs = model(images)\n",
        "\n",
        "        outputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n",
        "        model_time = time.time() - model_time\n",
        "\n",
        "        res = {target[\"image_id\"]: output for target, output in zip(targets, outputs)}\n",
        "        evaluator_time = time.time()\n",
        "        coco_evaluator.update(res)\n",
        "        evaluator_time = time.time() - evaluator_time\n",
        "        metric_logger.update(model_time=model_time, evaluator_time=evaluator_time)\n",
        "\n",
        "    # gather the stats from all processes\n",
        "    metric_logger.synchronize_between_processes()\n",
        "    print(\"Averaged stats:\", metric_logger)\n",
        "    coco_evaluator.synchronize_between_processes()\n",
        "\n",
        "    # accumulate predictions from all images\n",
        "    coco_evaluator.accumulate()\n",
        "    coco_evaluator.summarize()\n",
        "    torch.set_num_threads(n_threads)\n",
        "    return coco_evaluator"
      ],
      "metadata": {
        "id": "9fUoV3gjxVQ1"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import datetime\n",
        "import errno\n",
        "import os\n",
        "import time\n",
        "from collections import defaultdict, deque\n",
        "\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "\n",
        "\n",
        "class SmoothedValue:\n",
        "    \"\"\"Track a series of values and provide access to smoothed values over a\n",
        "    window or the global series average.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, window_size=20, fmt=None):\n",
        "        if fmt is None:\n",
        "            fmt = \"{median:.4f} ({global_avg:.4f})\"\n",
        "        self.deque = deque(maxlen=window_size)\n",
        "        self.total = 0.0\n",
        "        self.count = 0\n",
        "        self.fmt = fmt\n",
        "\n",
        "    def update(self, value, n=1):\n",
        "        self.deque.append(value)\n",
        "        self.count += n\n",
        "        self.total += value * n\n",
        "\n",
        "    def synchronize_between_processes(self):\n",
        "        \"\"\"\n",
        "        Warning: does not synchronize the deque!\n",
        "        \"\"\"\n",
        "        if not is_dist_avail_and_initialized():\n",
        "            return\n",
        "        t = torch.tensor([self.count, self.total], dtype=torch.float64, device=\"cuda\")\n",
        "        dist.barrier()\n",
        "        dist.all_reduce(t)\n",
        "        t = t.tolist()\n",
        "        self.count = int(t[0])\n",
        "        self.total = t[1]\n",
        "\n",
        "    @property\n",
        "    def median(self):\n",
        "        d = torch.tensor(list(self.deque))\n",
        "        return d.median().item()\n",
        "\n",
        "    @property\n",
        "    def avg(self):\n",
        "        d = torch.tensor(list(self.deque), dtype=torch.float32)\n",
        "        return d.mean().item()\n",
        "\n",
        "    @property\n",
        "    def global_avg(self):\n",
        "        return self.total / self.count\n",
        "\n",
        "    @property\n",
        "    def max(self):\n",
        "        return max(self.deque)\n",
        "\n",
        "    @property\n",
        "    def value(self):\n",
        "        return self.deque[-1]\n",
        "\n",
        "    def __str__(self):\n",
        "        return self.fmt.format(\n",
        "            median=self.median, avg=self.avg, global_avg=self.global_avg, max=self.max, value=self.value\n",
        "        )\n",
        "\n",
        "\n",
        "def all_gather(data):\n",
        "    \"\"\"\n",
        "    Run all_gather on arbitrary picklable data (not necessarily tensors)\n",
        "    Args:\n",
        "        data: any picklable object\n",
        "    Returns:\n",
        "        list[data]: list of data gathered from each rank\n",
        "    \"\"\"\n",
        "    world_size = get_world_size()\n",
        "    if world_size == 1:\n",
        "        return [data]\n",
        "    data_list = [None] * world_size\n",
        "    dist.all_gather_object(data_list, data)\n",
        "    return data_list\n",
        "\n",
        "\n",
        "def reduce_dict(input_dict, average=True):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "        input_dict (dict): all the values will be reduced\n",
        "        average (bool): whether to do average or sum\n",
        "    Reduce the values in the dictionary from all processes so that all processes\n",
        "    have the averaged results. Returns a dict with the same fields as\n",
        "    input_dict, after reduction.\n",
        "    \"\"\"\n",
        "    world_size = get_world_size()\n",
        "    if world_size < 2:\n",
        "        return input_dict\n",
        "    with torch.inference_mode():\n",
        "        names = []\n",
        "        values = []\n",
        "        # sort the keys so that they are consistent across processes\n",
        "        for k in sorted(input_dict.keys()):\n",
        "            names.append(k)\n",
        "            values.append(input_dict[k])\n",
        "        values = torch.stack(values, dim=0)\n",
        "        dist.all_reduce(values)\n",
        "        if average:\n",
        "            values /= world_size\n",
        "        reduced_dict = {k: v for k, v in zip(names, values)}\n",
        "    return reduced_dict\n",
        "\n",
        "\n",
        "class MetricLogger:\n",
        "    def __init__(self, delimiter=\"\\t\"):\n",
        "        self.meters = defaultdict(SmoothedValue)\n",
        "        self.delimiter = delimiter\n",
        "\n",
        "    def update(self, **kwargs):\n",
        "        for k, v in kwargs.items():\n",
        "            if isinstance(v, torch.Tensor):\n",
        "                v = v.item()\n",
        "            assert isinstance(v, (float, int))\n",
        "            self.meters[k].update(v)\n",
        "\n",
        "    def __getattr__(self, attr):\n",
        "        if attr in self.meters:\n",
        "            return self.meters[attr]\n",
        "        if attr in self.__dict__:\n",
        "            return self.__dict__[attr]\n",
        "        raise AttributeError(f\"'{type(self).__name__}' object has no attribute '{attr}'\")\n",
        "\n",
        "    def __str__(self):\n",
        "        loss_str = []\n",
        "        for name, meter in self.meters.items():\n",
        "            loss_str.append(f\"{name}: {str(meter)}\")\n",
        "        return self.delimiter.join(loss_str)\n",
        "\n",
        "    def synchronize_between_processes(self):\n",
        "        for meter in self.meters.values():\n",
        "            meter.synchronize_between_processes()\n",
        "\n",
        "    def add_meter(self, name, meter):\n",
        "        self.meters[name] = meter\n",
        "\n",
        "    def log_every(self, iterable, print_freq, header=None):\n",
        "        i = 0\n",
        "        if not header:\n",
        "            header = \"\"\n",
        "        start_time = time.time()\n",
        "        end = time.time()\n",
        "        iter_time = SmoothedValue(fmt=\"{avg:.4f}\")\n",
        "        data_time = SmoothedValue(fmt=\"{avg:.4f}\")\n",
        "        space_fmt = \":\" + str(len(str(len(iterable)))) + \"d\"\n",
        "        if torch.cuda.is_available():\n",
        "            log_msg = self.delimiter.join(\n",
        "                [\n",
        "                    header,\n",
        "                    \"[{0\" + space_fmt + \"}/{1}]\",\n",
        "                    \"eta: {eta}\",\n",
        "                    \"{meters}\",\n",
        "                    \"time: {time}\",\n",
        "                    \"data: {data}\",\n",
        "                    \"max mem: {memory:.0f}\",\n",
        "                ]\n",
        "            )\n",
        "        else:\n",
        "            log_msg = self.delimiter.join(\n",
        "                [header, \"[{0\" + space_fmt + \"}/{1}]\", \"eta: {eta}\", \"{meters}\", \"time: {time}\", \"data: {data}\"]\n",
        "            )\n",
        "        MB = 1024.0 * 1024.0\n",
        "        for obj in iterable:\n",
        "            data_time.update(time.time() - end)\n",
        "            yield obj\n",
        "            iter_time.update(time.time() - end)\n",
        "            if i % print_freq == 0 or i == len(iterable) - 1:\n",
        "                eta_seconds = iter_time.global_avg * (len(iterable) - i)\n",
        "                eta_string = str(datetime.timedelta(seconds=int(eta_seconds)))\n",
        "                if torch.cuda.is_available():\n",
        "                    print(\n",
        "                        log_msg.format(\n",
        "                            i,\n",
        "                            len(iterable),\n",
        "                            eta=eta_string,\n",
        "                            meters=str(self),\n",
        "                            time=str(iter_time),\n",
        "                            data=str(data_time),\n",
        "                            memory=torch.cuda.max_memory_allocated() / MB,\n",
        "                        )\n",
        "                    )\n",
        "                else:\n",
        "                    print(\n",
        "                        log_msg.format(\n",
        "                            i, len(iterable), eta=eta_string, meters=str(self), time=str(iter_time), data=str(data_time)\n",
        "                        )\n",
        "                    )\n",
        "            i += 1\n",
        "            end = time.time()\n",
        "        total_time = time.time() - start_time\n",
        "        total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
        "        print(f\"{header} Total time: {total_time_str} ({total_time / len(iterable):.4f} s / it)\")\n",
        "\n",
        "\n",
        "def collate_fn(batch):\n",
        "    return tuple(zip(*batch))\n",
        "\n",
        "\n",
        "def mkdir(path):\n",
        "    try:\n",
        "        os.makedirs(path)\n",
        "    except OSError as e:\n",
        "        if e.errno != errno.EEXIST:\n",
        "            raise\n",
        "\n",
        "\n",
        "def setup_for_distributed(is_master):\n",
        "    \"\"\"\n",
        "    This function disables printing when not in master process\n",
        "    \"\"\"\n",
        "    import builtins as __builtin__\n",
        "\n",
        "    builtin_print = __builtin__.print\n",
        "\n",
        "    def print(*args, **kwargs):\n",
        "        force = kwargs.pop(\"force\", False)\n",
        "        if is_master or force:\n",
        "            builtin_print(*args, **kwargs)\n",
        "\n",
        "    __builtin__.print = print\n",
        "\n",
        "\n",
        "def is_dist_avail_and_initialized():\n",
        "    if not dist.is_available():\n",
        "        return False\n",
        "    if not dist.is_initialized():\n",
        "        return False\n",
        "    return True\n",
        "\n",
        "\n",
        "def get_world_size():\n",
        "    if not is_dist_avail_and_initialized():\n",
        "        return 1\n",
        "    return dist.get_world_size()\n",
        "\n",
        "\n",
        "def get_rank():\n",
        "    if not is_dist_avail_and_initialized():\n",
        "        return 0\n",
        "    return dist.get_rank()\n",
        "\n",
        "\n",
        "def is_main_process():\n",
        "    return get_rank() == 0\n",
        "\n",
        "\n",
        "def save_on_master(*args, **kwargs):\n",
        "    if is_main_process():\n",
        "        torch.save(*args, **kwargs)\n",
        "\n",
        "\n",
        "def init_distributed_mode(args):\n",
        "    if \"RANK\" in os.environ and \"WORLD_SIZE\" in os.environ:\n",
        "        args.rank = int(os.environ[\"RANK\"])\n",
        "        args.world_size = int(os.environ[\"WORLD_SIZE\"])\n",
        "        args.gpu = int(os.environ[\"LOCAL_RANK\"])\n",
        "    elif \"SLURM_PROCID\" in os.environ:\n",
        "        args.rank = int(os.environ[\"SLURM_PROCID\"])\n",
        "        args.gpu = args.rank % torch.cuda.device_count()\n",
        "    else:\n",
        "        print(\"Not using distributed mode\")\n",
        "        args.distributed = False\n",
        "        return\n",
        "\n",
        "    args.distributed = True\n",
        "\n",
        "    torch.cuda.set_device(args.gpu)\n",
        "    args.dist_backend = \"nccl\"\n",
        "    print(f\"| distributed init (rank {args.rank}): {args.dist_url}\", flush=True)\n",
        "    torch.distributed.init_process_group(\n",
        "        backend=args.dist_backend, init_method=args.dist_url, world_size=args.world_size, rank=args.rank\n",
        "    )\n",
        "    torch.distributed.barrier()\n",
        "    setup_for_distributed(args.rank == 0)"
      ],
      "metadata": {
        "id": "rp2_lB7SxdXk"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.utils.data\n",
        "import torchvision\n",
        "import transforms as T\n",
        "from pycocotools import mask as coco_mask\n",
        "from pycocotools.coco import COCO\n",
        "\n",
        "\n",
        "def convert_coco_poly_to_mask(segmentations, height, width):\n",
        "    masks = []\n",
        "    for polygons in segmentations:\n",
        "        rles = coco_mask.frPyObjects(polygons, height, width)\n",
        "        mask = coco_mask.decode(rles)\n",
        "        if len(mask.shape) < 3:\n",
        "            mask = mask[..., None]\n",
        "        mask = torch.as_tensor(mask, dtype=torch.uint8)\n",
        "        mask = mask.any(dim=2)\n",
        "        masks.append(mask)\n",
        "    if masks:\n",
        "        masks = torch.stack(masks, dim=0)\n",
        "    else:\n",
        "        masks = torch.zeros((0, height, width), dtype=torch.uint8)\n",
        "    return masks\n",
        "\n",
        "\n",
        "class ConvertCocoPolysToMask:\n",
        "    def __call__(self, image, target):\n",
        "        w, h = image.size\n",
        "\n",
        "        image_id = target[\"image_id\"]\n",
        "\n",
        "        anno = target[\"annotations\"]\n",
        "\n",
        "        anno = [obj for obj in anno if obj[\"iscrowd\"] == 0]\n",
        "\n",
        "        boxes = [obj[\"bbox\"] for obj in anno]\n",
        "        # guard against no boxes via resizing\n",
        "        boxes = torch.as_tensor(boxes, dtype=torch.float32).reshape(-1, 4)\n",
        "        boxes[:, 2:] += boxes[:, :2]\n",
        "        boxes[:, 0::2].clamp_(min=0, max=w)\n",
        "        boxes[:, 1::2].clamp_(min=0, max=h)\n",
        "\n",
        "        classes = [obj[\"category_id\"] for obj in anno]\n",
        "        classes = torch.tensor(classes, dtype=torch.int64)\n",
        "\n",
        "        segmentations = [obj[\"segmentation\"] for obj in anno]\n",
        "        masks = convert_coco_poly_to_mask(segmentations, h, w)\n",
        "\n",
        "        keypoints = None\n",
        "        if anno and \"keypoints\" in anno[0]:\n",
        "            keypoints = [obj[\"keypoints\"] for obj in anno]\n",
        "            keypoints = torch.as_tensor(keypoints, dtype=torch.float32)\n",
        "            num_keypoints = keypoints.shape[0]\n",
        "            if num_keypoints:\n",
        "                keypoints = keypoints.view(num_keypoints, -1, 3)\n",
        "\n",
        "        keep = (boxes[:, 3] > boxes[:, 1]) & (boxes[:, 2] > boxes[:, 0])\n",
        "        boxes = boxes[keep]\n",
        "        classes = classes[keep]\n",
        "        masks = masks[keep]\n",
        "        if keypoints is not None:\n",
        "            keypoints = keypoints[keep]\n",
        "\n",
        "        target = {}\n",
        "        target[\"boxes\"] = boxes\n",
        "        target[\"labels\"] = classes\n",
        "        target[\"masks\"] = masks\n",
        "        target[\"image_id\"] = image_id\n",
        "        if keypoints is not None:\n",
        "            target[\"keypoints\"] = keypoints\n",
        "\n",
        "        # for conversion to coco api\n",
        "        area = torch.tensor([obj[\"area\"] for obj in anno])\n",
        "        iscrowd = torch.tensor([obj[\"iscrowd\"] for obj in anno])\n",
        "        target[\"area\"] = area\n",
        "        target[\"iscrowd\"] = iscrowd\n",
        "\n",
        "        return image, target\n",
        "\n",
        "\n",
        "def _coco_remove_images_without_annotations(dataset, cat_list=None):\n",
        "    def _has_only_empty_bbox(anno):\n",
        "        return all(any(o <= 1 for o in obj[\"bbox\"][2:]) for obj in anno)\n",
        "\n",
        "    def _count_visible_keypoints(anno):\n",
        "        return sum(sum(1 for v in ann[\"keypoints\"][2::3] if v > 0) for ann in anno)\n",
        "\n",
        "    min_keypoints_per_image = 10\n",
        "\n",
        "    def _has_valid_annotation(anno):\n",
        "        # if it's empty, there is no annotation\n",
        "        if len(anno) == 0:\n",
        "            return False\n",
        "        # if all boxes have close to zero area, there is no annotation\n",
        "        if _has_only_empty_bbox(anno):\n",
        "            return False\n",
        "        # keypoints task have a slight different criteria for considering\n",
        "        # if an annotation is valid\n",
        "        if \"keypoints\" not in anno[0]:\n",
        "            return True\n",
        "        # for keypoint detection tasks, only consider valid images those\n",
        "        # containing at least min_keypoints_per_image\n",
        "        if _count_visible_keypoints(anno) >= min_keypoints_per_image:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    ids = []\n",
        "    for ds_idx, img_id in enumerate(dataset.ids):\n",
        "        ann_ids = dataset.coco.getAnnIds(imgIds=img_id, iscrowd=None)\n",
        "        anno = dataset.coco.loadAnns(ann_ids)\n",
        "        if cat_list:\n",
        "            anno = [obj for obj in anno if obj[\"category_id\"] in cat_list]\n",
        "        if _has_valid_annotation(anno):\n",
        "            ids.append(ds_idx)\n",
        "\n",
        "    dataset = torch.utils.data.Subset(dataset, ids)\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def convert_to_coco_api(ds):\n",
        "    coco_ds = COCO()\n",
        "    # annotation IDs need to start at 1, not 0, see torchvision issue #1530\n",
        "    ann_id = 1\n",
        "    dataset = {\"images\": [], \"categories\": [], \"annotations\": []}\n",
        "    categories = set()\n",
        "    for img_idx in range(len(ds)):\n",
        "        # find better way to get target\n",
        "        # targets = ds.get_annotations(img_idx)\n",
        "        img, targets = ds[img_idx]\n",
        "        image_id = targets[\"image_id\"]\n",
        "        img_dict = {}\n",
        "        img_dict[\"id\"] = image_id\n",
        "        img_dict[\"height\"] = img.shape[-2]\n",
        "        img_dict[\"width\"] = img.shape[-1]\n",
        "        dataset[\"images\"].append(img_dict)\n",
        "        bboxes = targets[\"boxes\"].clone()\n",
        "        bboxes[:, 2:] -= bboxes[:, :2]\n",
        "        bboxes = bboxes.tolist()\n",
        "        labels = targets[\"labels\"].tolist()\n",
        "        areas = targets[\"area\"].tolist()\n",
        "        iscrowd = targets[\"iscrowd\"].tolist()\n",
        "        if \"masks\" in targets:\n",
        "            masks = targets[\"masks\"]\n",
        "            # make masks Fortran contiguous for coco_mask\n",
        "            masks = masks.permute(0, 2, 1).contiguous().permute(0, 2, 1)\n",
        "        if \"keypoints\" in targets:\n",
        "            keypoints = targets[\"keypoints\"]\n",
        "            keypoints = keypoints.reshape(keypoints.shape[0], -1).tolist()\n",
        "        num_objs = len(bboxes)\n",
        "        for i in range(num_objs):\n",
        "            ann = {}\n",
        "            ann[\"image_id\"] = image_id\n",
        "            ann[\"bbox\"] = bboxes[i]\n",
        "            ann[\"category_id\"] = labels[i]\n",
        "            categories.add(labels[i])\n",
        "            ann[\"area\"] = areas[i]\n",
        "            ann[\"iscrowd\"] = iscrowd[i]\n",
        "            ann[\"id\"] = ann_id\n",
        "            if \"masks\" in targets:\n",
        "                ann[\"segmentation\"] = coco_mask.encode(masks[i].numpy())\n",
        "            if \"keypoints\" in targets:\n",
        "                ann[\"keypoints\"] = keypoints[i]\n",
        "                ann[\"num_keypoints\"] = sum(k != 0 for k in keypoints[i][2::3])\n",
        "            dataset[\"annotations\"].append(ann)\n",
        "            ann_id += 1\n",
        "    dataset[\"categories\"] = [{\"id\": i} for i in sorted(categories)]\n",
        "    coco_ds.dataset = dataset\n",
        "    coco_ds.createIndex()\n",
        "    return coco_ds\n",
        "\n",
        "\n",
        "def get_coco_api_from_dataset(dataset):\n",
        "    # FIXME: This is... awful?\n",
        "    for _ in range(10):\n",
        "        if isinstance(dataset, torchvision.datasets.CocoDetection):\n",
        "            break\n",
        "        if isinstance(dataset, torch.utils.data.Subset):\n",
        "            dataset = dataset.dataset\n",
        "    if isinstance(dataset, torchvision.datasets.CocoDetection):\n",
        "        return dataset.coco\n",
        "    return convert_to_coco_api(dataset)\n",
        "\n",
        "\n",
        "class CocoDetection(torchvision.datasets.CocoDetection):\n",
        "    def __init__(self, img_folder, ann_file, transforms):\n",
        "        super().__init__(img_folder, ann_file)\n",
        "        self._transforms = transforms\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img, target = super().__getitem__(idx)\n",
        "        image_id = self.ids[idx]\n",
        "        target = dict(image_id=image_id, annotations=target)\n",
        "        if self._transforms is not None:\n",
        "            img, target = self._transforms(img, target)\n",
        "        return img, target\n",
        "\n",
        "\n",
        "def get_coco(root, image_set, transforms, mode=\"instances\", use_v2=False, with_masks=False):\n",
        "    anno_file_template = \"{}_{}2017.json\"\n",
        "    PATHS = {\n",
        "        \"train\": (\"train2017\", os.path.join(\"annotations\", anno_file_template.format(mode, \"train\"))),\n",
        "        \"val\": (\"val2017\", os.path.join(\"annotations\", anno_file_template.format(mode, \"val\"))),\n",
        "        # \"train\": (\"val2017\", os.path.join(\"annotations\", anno_file_template.format(mode, \"val\")))\n",
        "    }\n",
        "\n",
        "    img_folder, ann_file = PATHS[image_set]\n",
        "    img_folder = os.path.join(root, img_folder)\n",
        "    ann_file = os.path.join(root, ann_file)\n",
        "\n",
        "    if use_v2:\n",
        "        from torchvision.datasets import wrap_dataset_for_transforms_v2\n",
        "\n",
        "        dataset = torchvision.datasets.CocoDetection(img_folder, ann_file, transforms=transforms)\n",
        "        target_keys = [\"boxes\", \"labels\", \"image_id\"]\n",
        "        if with_masks:\n",
        "            target_keys += [\"masks\"]\n",
        "        dataset = wrap_dataset_for_transforms_v2(dataset, target_keys=target_keys)\n",
        "    else:\n",
        "        # TODO: handle with_masks for V1?\n",
        "        t = [ConvertCocoPolysToMask()]\n",
        "        if transforms is not None:\n",
        "            t.append(transforms)\n",
        "        transforms = T.Compose(t)\n",
        "\n",
        "        dataset = CocoDetection(img_folder, ann_file, transforms=transforms)\n",
        "\n",
        "    if image_set == \"train\":\n",
        "        dataset = _coco_remove_images_without_annotations(dataset)\n",
        "\n",
        "    # dataset = torch.utils.data.Subset(dataset, [i for i in range(500)])\n",
        "\n",
        "    return dataset"
      ],
      "metadata": {
        "id": "IZ5ZA1lQxlie"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import copy\n",
        "import io\n",
        "from contextlib import redirect_stdout\n",
        "\n",
        "import numpy as np\n",
        "import pycocotools.mask as mask_util\n",
        "import torch\n",
        "import utils\n",
        "from pycocotools.coco import COCO\n",
        "from pycocotools.cocoeval import COCOeval\n",
        "\n",
        "\n",
        "class CocoEvaluator:\n",
        "    def __init__(self, coco_gt, iou_types):\n",
        "        if not isinstance(iou_types, (list, tuple)):\n",
        "            raise TypeError(f\"This constructor expects iou_types of type list or tuple, instead  got {type(iou_types)}\")\n",
        "        coco_gt = copy.deepcopy(coco_gt)\n",
        "        self.coco_gt = coco_gt\n",
        "\n",
        "        self.iou_types = iou_types\n",
        "        self.coco_eval = {}\n",
        "        for iou_type in iou_types:\n",
        "            self.coco_eval[iou_type] = COCOeval(coco_gt, iouType=iou_type)\n",
        "\n",
        "        self.img_ids = []\n",
        "        self.eval_imgs = {k: [] for k in iou_types}\n",
        "\n",
        "    def update(self, predictions):\n",
        "        img_ids = list(np.unique(list(predictions.keys())))\n",
        "        self.img_ids.extend(img_ids)\n",
        "\n",
        "        for iou_type in self.iou_types:\n",
        "            results = self.prepare(predictions, iou_type)\n",
        "            with redirect_stdout(io.StringIO()):\n",
        "                coco_dt = COCO.loadRes(self.coco_gt, results) if results else COCO()\n",
        "            coco_eval = self.coco_eval[iou_type]\n",
        "\n",
        "            coco_eval.cocoDt = coco_dt\n",
        "            coco_eval.params.imgIds = list(img_ids)\n",
        "            img_ids, eval_imgs = evaluate(coco_eval)\n",
        "\n",
        "            self.eval_imgs[iou_type].append(eval_imgs)\n",
        "\n",
        "    def synchronize_between_processes(self):\n",
        "        for iou_type in self.iou_types:\n",
        "            self.eval_imgs[iou_type] = np.concatenate(self.eval_imgs[iou_type], 2)\n",
        "            create_common_coco_eval(self.coco_eval[iou_type], self.img_ids, self.eval_imgs[iou_type])\n",
        "\n",
        "    def accumulate(self):\n",
        "        for coco_eval in self.coco_eval.values():\n",
        "            coco_eval.accumulate()\n",
        "\n",
        "    def summarize(self):\n",
        "        for iou_type, coco_eval in self.coco_eval.items():\n",
        "            print(f\"IoU metric: {iou_type}\")\n",
        "            coco_eval.summarize()\n",
        "\n",
        "    def prepare(self, predictions, iou_type):\n",
        "        if iou_type == \"bbox\":\n",
        "            return self.prepare_for_coco_detection(predictions)\n",
        "        if iou_type == \"segm\":\n",
        "            return self.prepare_for_coco_segmentation(predictions)\n",
        "        if iou_type == \"keypoints\":\n",
        "            return self.prepare_for_coco_keypoint(predictions)\n",
        "        raise ValueError(f\"Unknown iou type {iou_type}\")\n",
        "\n",
        "    def prepare_for_coco_detection(self, predictions):\n",
        "        coco_results = []\n",
        "        for original_id, prediction in predictions.items():\n",
        "            if len(prediction) == 0:\n",
        "                continue\n",
        "\n",
        "            boxes = prediction[\"boxes\"]\n",
        "            boxes = convert_to_xywh(boxes).tolist()\n",
        "            scores = prediction[\"scores\"].tolist()\n",
        "            labels = prediction[\"labels\"].tolist()\n",
        "\n",
        "            coco_results.extend(\n",
        "                [\n",
        "                    {\n",
        "                        \"image_id\": original_id,\n",
        "                        \"category_id\": labels[k],\n",
        "                        \"bbox\": box,\n",
        "                        \"score\": scores[k],\n",
        "                    }\n",
        "                    for k, box in enumerate(boxes)\n",
        "                ]\n",
        "            )\n",
        "        return coco_results\n",
        "\n",
        "    def prepare_for_coco_segmentation(self, predictions):\n",
        "        coco_results = []\n",
        "        for original_id, prediction in predictions.items():\n",
        "            if len(prediction) == 0:\n",
        "                continue\n",
        "\n",
        "            scores = prediction[\"scores\"]\n",
        "            labels = prediction[\"labels\"]\n",
        "            masks = prediction[\"masks\"]\n",
        "\n",
        "            masks = masks > 0.5\n",
        "\n",
        "            scores = prediction[\"scores\"].tolist()\n",
        "            labels = prediction[\"labels\"].tolist()\n",
        "\n",
        "            rles = [\n",
        "                mask_util.encode(np.array(mask[0, :, :, np.newaxis], dtype=np.uint8, order=\"F\"))[0] for mask in masks\n",
        "            ]\n",
        "            for rle in rles:\n",
        "                rle[\"counts\"] = rle[\"counts\"].decode(\"utf-8\")\n",
        "\n",
        "            coco_results.extend(\n",
        "                [\n",
        "                    {\n",
        "                        \"image_id\": original_id,\n",
        "                        \"category_id\": labels[k],\n",
        "                        \"segmentation\": rle,\n",
        "                        \"score\": scores[k],\n",
        "                    }\n",
        "                    for k, rle in enumerate(rles)\n",
        "                ]\n",
        "            )\n",
        "        return coco_results\n",
        "\n",
        "    def prepare_for_coco_keypoint(self, predictions):\n",
        "        coco_results = []\n",
        "        for original_id, prediction in predictions.items():\n",
        "            if len(prediction) == 0:\n",
        "                continue\n",
        "\n",
        "            boxes = prediction[\"boxes\"]\n",
        "            boxes = convert_to_xywh(boxes).tolist()\n",
        "            scores = prediction[\"scores\"].tolist()\n",
        "            labels = prediction[\"labels\"].tolist()\n",
        "            keypoints = prediction[\"keypoints\"]\n",
        "            keypoints = keypoints.flatten(start_dim=1).tolist()\n",
        "\n",
        "            coco_results.extend(\n",
        "                [\n",
        "                    {\n",
        "                        \"image_id\": original_id,\n",
        "                        \"category_id\": labels[k],\n",
        "                        \"keypoints\": keypoint,\n",
        "                        \"score\": scores[k],\n",
        "                    }\n",
        "                    for k, keypoint in enumerate(keypoints)\n",
        "                ]\n",
        "            )\n",
        "        return coco_results\n",
        "\n",
        "\n",
        "def convert_to_xywh(boxes):\n",
        "    xmin, ymin, xmax, ymax = boxes.unbind(1)\n",
        "    return torch.stack((xmin, ymin, xmax - xmin, ymax - ymin), dim=1)\n",
        "\n",
        "\n",
        "def merge(img_ids, eval_imgs):\n",
        "    all_img_ids = utils.all_gather(img_ids)\n",
        "    all_eval_imgs = utils.all_gather(eval_imgs)\n",
        "\n",
        "    merged_img_ids = []\n",
        "    for p in all_img_ids:\n",
        "        merged_img_ids.extend(p)\n",
        "\n",
        "    merged_eval_imgs = []\n",
        "    for p in all_eval_imgs:\n",
        "        merged_eval_imgs.append(p)\n",
        "\n",
        "    merged_img_ids = np.array(merged_img_ids)\n",
        "    merged_eval_imgs = np.concatenate(merged_eval_imgs, 2)\n",
        "\n",
        "    # keep only unique (and in sorted order) images\n",
        "    merged_img_ids, idx = np.unique(merged_img_ids, return_index=True)\n",
        "    merged_eval_imgs = merged_eval_imgs[..., idx]\n",
        "\n",
        "    return merged_img_ids, merged_eval_imgs\n",
        "\n",
        "\n",
        "def create_common_coco_eval(coco_eval, img_ids, eval_imgs):\n",
        "    img_ids, eval_imgs = merge(img_ids, eval_imgs)\n",
        "    img_ids = list(img_ids)\n",
        "    eval_imgs = list(eval_imgs.flatten())\n",
        "\n",
        "    coco_eval.evalImgs = eval_imgs\n",
        "    coco_eval.params.imgIds = img_ids\n",
        "    coco_eval._paramsEval = copy.deepcopy(coco_eval.params)\n",
        "\n",
        "\n",
        "def evaluate(imgs):\n",
        "    with redirect_stdout(io.StringIO()):\n",
        "        imgs.evaluate()\n",
        "    return imgs.params.imgIds, np.asarray(imgs.evalImgs).reshape(-1, len(imgs.params.areaRng), len(imgs.params.imgIds))"
      ],
      "metadata": {
        "id": "vnbrBCmixrmP"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Dict, List, Optional, Tuple, Union\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torch import nn, Tensor\n",
        "from torchvision import ops\n",
        "from torchvision.transforms import functional as F, InterpolationMode, transforms as T\n",
        "\n",
        "\n",
        "def _flip_coco_person_keypoints(kps, width):\n",
        "    flip_inds = [0, 2, 1, 4, 3, 6, 5, 8, 7, 10, 9, 12, 11, 14, 13, 16, 15]\n",
        "    flipped_data = kps[:, flip_inds]\n",
        "    flipped_data[..., 0] = width - flipped_data[..., 0]\n",
        "    # Maintain COCO convention that if visibility == 0, then x, y = 0\n",
        "    inds = flipped_data[..., 2] == 0\n",
        "    flipped_data[inds] = 0\n",
        "    return flipped_data\n",
        "\n",
        "\n",
        "class Compose:\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, image, target):\n",
        "        for t in self.transforms:\n",
        "            image, target = t(image, target)\n",
        "        return image, target\n",
        "\n",
        "\n",
        "class RandomHorizontalFlip(T.RandomHorizontalFlip):\n",
        "    def forward(\n",
        "        self, image: Tensor, target: Optional[Dict[str, Tensor]] = None\n",
        "    ) -> Tuple[Tensor, Optional[Dict[str, Tensor]]]:\n",
        "        if torch.rand(1) < self.p:\n",
        "            image = F.hflip(image)\n",
        "            if target is not None:\n",
        "                _, _, width = F.get_dimensions(image)\n",
        "                target[\"boxes\"][:, [0, 2]] = width - target[\"boxes\"][:, [2, 0]]\n",
        "                if \"masks\" in target:\n",
        "                    target[\"masks\"] = target[\"masks\"].flip(-1)\n",
        "                if \"keypoints\" in target:\n",
        "                    keypoints = target[\"keypoints\"]\n",
        "                    keypoints = _flip_coco_person_keypoints(keypoints, width)\n",
        "                    target[\"keypoints\"] = keypoints\n",
        "        return image, target\n",
        "\n",
        "\n",
        "class PILToTensor(nn.Module):\n",
        "    def forward(\n",
        "        self, image: Tensor, target: Optional[Dict[str, Tensor]] = None\n",
        "    ) -> Tuple[Tensor, Optional[Dict[str, Tensor]]]:\n",
        "        image = F.pil_to_tensor(image)\n",
        "        return image, target\n",
        "\n",
        "\n",
        "class ToDtype(nn.Module):\n",
        "    def __init__(self, dtype: torch.dtype, scale: bool = False) -> None:\n",
        "        super().__init__()\n",
        "        self.dtype = dtype\n",
        "        self.scale = scale\n",
        "\n",
        "    def forward(\n",
        "        self, image: Tensor, target: Optional[Dict[str, Tensor]] = None\n",
        "    ) -> Tuple[Tensor, Optional[Dict[str, Tensor]]]:\n",
        "        if not self.scale:\n",
        "            return image.to(dtype=self.dtype), target\n",
        "        image = F.convert_image_dtype(image, self.dtype)\n",
        "        return image, target\n",
        "\n",
        "\n",
        "class RandomIoUCrop(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        min_scale: float = 0.3,\n",
        "        max_scale: float = 1.0,\n",
        "        min_aspect_ratio: float = 0.5,\n",
        "        max_aspect_ratio: float = 2.0,\n",
        "        sampler_options: Optional[List[float]] = None,\n",
        "        trials: int = 40,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        # Configuration similar to https://github.com/weiliu89/caffe/blob/ssd/examples/ssd/ssd_coco.py#L89-L174\n",
        "        self.min_scale = min_scale\n",
        "        self.max_scale = max_scale\n",
        "        self.min_aspect_ratio = min_aspect_ratio\n",
        "        self.max_aspect_ratio = max_aspect_ratio\n",
        "        if sampler_options is None:\n",
        "            sampler_options = [0.0, 0.1, 0.3, 0.5, 0.7, 0.9, 1.0]\n",
        "        self.options = sampler_options\n",
        "        self.trials = trials\n",
        "\n",
        "    def forward(\n",
        "        self, image: Tensor, target: Optional[Dict[str, Tensor]] = None\n",
        "    ) -> Tuple[Tensor, Optional[Dict[str, Tensor]]]:\n",
        "        if target is None:\n",
        "            raise ValueError(\"The targets can't be None for this transform.\")\n",
        "\n",
        "        if isinstance(image, torch.Tensor):\n",
        "            if image.ndimension() not in {2, 3}:\n",
        "                raise ValueError(f\"image should be 2/3 dimensional. Got {image.ndimension()} dimensions.\")\n",
        "            elif image.ndimension() == 2:\n",
        "                image = image.unsqueeze(0)\n",
        "\n",
        "        _, orig_h, orig_w = F.get_dimensions(image)\n",
        "\n",
        "        while True:\n",
        "            # sample an option\n",
        "            idx = int(torch.randint(low=0, high=len(self.options), size=(1,)))\n",
        "            min_jaccard_overlap = self.options[idx]\n",
        "            if min_jaccard_overlap >= 1.0:  # a value larger than 1 encodes the leave as-is option\n",
        "                return image, target\n",
        "\n",
        "            for _ in range(self.trials):\n",
        "                # check the aspect ratio limitations\n",
        "                r = self.min_scale + (self.max_scale - self.min_scale) * torch.rand(2)\n",
        "                new_w = int(orig_w * r[0])\n",
        "                new_h = int(orig_h * r[1])\n",
        "                aspect_ratio = new_w / new_h\n",
        "                if not (self.min_aspect_ratio <= aspect_ratio <= self.max_aspect_ratio):\n",
        "                    continue\n",
        "\n",
        "                # check for 0 area crops\n",
        "                r = torch.rand(2)\n",
        "                left = int((orig_w - new_w) * r[0])\n",
        "                top = int((orig_h - new_h) * r[1])\n",
        "                right = left + new_w\n",
        "                bottom = top + new_h\n",
        "                if left == right or top == bottom:\n",
        "                    continue\n",
        "\n",
        "                # check for any valid boxes with centers within the crop area\n",
        "                cx = 0.5 * (target[\"boxes\"][:, 0] + target[\"boxes\"][:, 2])\n",
        "                cy = 0.5 * (target[\"boxes\"][:, 1] + target[\"boxes\"][:, 3])\n",
        "                is_within_crop_area = (left < cx) & (cx < right) & (top < cy) & (cy < bottom)\n",
        "                if not is_within_crop_area.any():\n",
        "                    continue\n",
        "\n",
        "                # check at least 1 box with jaccard limitations\n",
        "                boxes = target[\"boxes\"][is_within_crop_area]\n",
        "                ious = torchvision.ops.boxes.box_iou(\n",
        "                    boxes, torch.tensor([[left, top, right, bottom]], dtype=boxes.dtype, device=boxes.device)\n",
        "                )\n",
        "                if ious.max() < min_jaccard_overlap:\n",
        "                    continue\n",
        "\n",
        "                # keep only valid boxes and perform cropping\n",
        "                target[\"boxes\"] = boxes\n",
        "                target[\"labels\"] = target[\"labels\"][is_within_crop_area]\n",
        "                target[\"boxes\"][:, 0::2] -= left\n",
        "                target[\"boxes\"][:, 1::2] -= top\n",
        "                target[\"boxes\"][:, 0::2].clamp_(min=0, max=new_w)\n",
        "                target[\"boxes\"][:, 1::2].clamp_(min=0, max=new_h)\n",
        "                image = F.crop(image, top, left, new_h, new_w)\n",
        "\n",
        "                return image, target\n",
        "\n",
        "\n",
        "class RandomZoomOut(nn.Module):\n",
        "    def __init__(\n",
        "        self, fill: Optional[List[float]] = None, side_range: Tuple[float, float] = (1.0, 4.0), p: float = 0.5\n",
        "    ):\n",
        "        super().__init__()\n",
        "        if fill is None:\n",
        "            fill = [0.0, 0.0, 0.0]\n",
        "        self.fill = fill\n",
        "        self.side_range = side_range\n",
        "        if side_range[0] < 1.0 or side_range[0] > side_range[1]:\n",
        "            raise ValueError(f\"Invalid canvas side range provided {side_range}.\")\n",
        "        self.p = p\n",
        "\n",
        "    @torch.jit.unused\n",
        "    def _get_fill_value(self, is_pil):\n",
        "        # type: (bool) -> int\n",
        "        # We fake the type to make it work on JIT\n",
        "        return tuple(int(x) for x in self.fill) if is_pil else 0\n",
        "\n",
        "    def forward(\n",
        "        self, image: Tensor, target: Optional[Dict[str, Tensor]] = None\n",
        "    ) -> Tuple[Tensor, Optional[Dict[str, Tensor]]]:\n",
        "        if isinstance(image, torch.Tensor):\n",
        "            if image.ndimension() not in {2, 3}:\n",
        "                raise ValueError(f\"image should be 2/3 dimensional. Got {image.ndimension()} dimensions.\")\n",
        "            elif image.ndimension() == 2:\n",
        "                image = image.unsqueeze(0)\n",
        "\n",
        "        if torch.rand(1) >= self.p:\n",
        "            return image, target\n",
        "\n",
        "        _, orig_h, orig_w = F.get_dimensions(image)\n",
        "\n",
        "        r = self.side_range[0] + torch.rand(1) * (self.side_range[1] - self.side_range[0])\n",
        "        canvas_width = int(orig_w * r)\n",
        "        canvas_height = int(orig_h * r)\n",
        "\n",
        "        r = torch.rand(2)\n",
        "        left = int((canvas_width - orig_w) * r[0])\n",
        "        top = int((canvas_height - orig_h) * r[1])\n",
        "        right = canvas_width - (left + orig_w)\n",
        "        bottom = canvas_height - (top + orig_h)\n",
        "\n",
        "        if torch.jit.is_scripting():\n",
        "            fill = 0\n",
        "        else:\n",
        "            fill = self._get_fill_value(F._is_pil_image(image))\n",
        "\n",
        "        image = F.pad(image, [left, top, right, bottom], fill=fill)\n",
        "        if isinstance(image, torch.Tensor):\n",
        "            # PyTorch's pad supports only integers on fill. So we need to overwrite the colour\n",
        "            v = torch.tensor(self.fill, device=image.device, dtype=image.dtype).view(-1, 1, 1)\n",
        "            image[..., :top, :] = image[..., :, :left] = image[..., (top + orig_h) :, :] = image[\n",
        "                ..., :, (left + orig_w) :\n",
        "            ] = v\n",
        "\n",
        "        if target is not None:\n",
        "            target[\"boxes\"][:, 0::2] += left\n",
        "            target[\"boxes\"][:, 1::2] += top\n",
        "\n",
        "        return image, target\n",
        "\n",
        "\n",
        "class RandomPhotometricDistort(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        contrast: Tuple[float, float] = (0.5, 1.5),\n",
        "        saturation: Tuple[float, float] = (0.5, 1.5),\n",
        "        hue: Tuple[float, float] = (-0.05, 0.05),\n",
        "        brightness: Tuple[float, float] = (0.875, 1.125),\n",
        "        p: float = 0.5,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self._brightness = T.ColorJitter(brightness=brightness)\n",
        "        self._contrast = T.ColorJitter(contrast=contrast)\n",
        "        self._hue = T.ColorJitter(hue=hue)\n",
        "        self._saturation = T.ColorJitter(saturation=saturation)\n",
        "        self.p = p\n",
        "\n",
        "    def forward(\n",
        "        self, image: Tensor, target: Optional[Dict[str, Tensor]] = None\n",
        "    ) -> Tuple[Tensor, Optional[Dict[str, Tensor]]]:\n",
        "        if isinstance(image, torch.Tensor):\n",
        "            if image.ndimension() not in {2, 3}:\n",
        "                raise ValueError(f\"image should be 2/3 dimensional. Got {image.ndimension()} dimensions.\")\n",
        "            elif image.ndimension() == 2:\n",
        "                image = image.unsqueeze(0)\n",
        "\n",
        "        r = torch.rand(7)\n",
        "\n",
        "        if r[0] < self.p:\n",
        "            image = self._brightness(image)\n",
        "\n",
        "        contrast_before = r[1] < 0.5\n",
        "        if contrast_before:\n",
        "            if r[2] < self.p:\n",
        "                image = self._contrast(image)\n",
        "\n",
        "        if r[3] < self.p:\n",
        "            image = self._saturation(image)\n",
        "\n",
        "        if r[4] < self.p:\n",
        "            image = self._hue(image)\n",
        "\n",
        "        if not contrast_before:\n",
        "            if r[5] < self.p:\n",
        "                image = self._contrast(image)\n",
        "\n",
        "        if r[6] < self.p:\n",
        "            channels, _, _ = F.get_dimensions(image)\n",
        "            permutation = torch.randperm(channels)\n",
        "\n",
        "            is_pil = F._is_pil_image(image)\n",
        "            if is_pil:\n",
        "                image = F.pil_to_tensor(image)\n",
        "                image = F.convert_image_dtype(image)\n",
        "            image = image[..., permutation, :, :]\n",
        "            if is_pil:\n",
        "                image = F.to_pil_image(image)\n",
        "\n",
        "        return image, target\n",
        "\n",
        "\n",
        "class ScaleJitter(nn.Module):\n",
        "    \"\"\"Randomly resizes the image and its bounding boxes  within the specified scale range.\n",
        "    The class implements the Scale Jitter augmentation as described in the paper\n",
        "    `\"Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation\" <https://arxiv.org/abs/2012.07177>`_.\n",
        "\n",
        "    Args:\n",
        "        target_size (tuple of ints): The target size for the transform provided in (height, weight) format.\n",
        "        scale_range (tuple of ints): scaling factor interval, e.g (a, b), then scale is randomly sampled from the\n",
        "            range a <= scale <= b.\n",
        "        interpolation (InterpolationMode): Desired interpolation enum defined by\n",
        "            :class:`torchvision.transforms.InterpolationMode`. Default is ``InterpolationMode.BILINEAR``.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        target_size: Tuple[int, int],\n",
        "        scale_range: Tuple[float, float] = (0.1, 2.0),\n",
        "        interpolation: InterpolationMode = InterpolationMode.BILINEAR,\n",
        "        antialias=True,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.target_size = target_size\n",
        "        self.scale_range = scale_range\n",
        "        self.interpolation = interpolation\n",
        "        self.antialias = antialias\n",
        "\n",
        "    def forward(\n",
        "        self, image: Tensor, target: Optional[Dict[str, Tensor]] = None\n",
        "    ) -> Tuple[Tensor, Optional[Dict[str, Tensor]]]:\n",
        "        if isinstance(image, torch.Tensor):\n",
        "            if image.ndimension() not in {2, 3}:\n",
        "                raise ValueError(f\"image should be 2/3 dimensional. Got {image.ndimension()} dimensions.\")\n",
        "            elif image.ndimension() == 2:\n",
        "                image = image.unsqueeze(0)\n",
        "\n",
        "        _, orig_height, orig_width = F.get_dimensions(image)\n",
        "\n",
        "        scale = self.scale_range[0] + torch.rand(1) * (self.scale_range[1] - self.scale_range[0])\n",
        "        r = min(self.target_size[1] / orig_height, self.target_size[0] / orig_width) * scale\n",
        "        new_width = int(orig_width * r)\n",
        "        new_height = int(orig_height * r)\n",
        "\n",
        "        image = F.resize(image, [new_height, new_width], interpolation=self.interpolation, antialias=self.antialias)\n",
        "\n",
        "        if target is not None:\n",
        "            target[\"boxes\"][:, 0::2] *= new_width / orig_width\n",
        "            target[\"boxes\"][:, 1::2] *= new_height / orig_height\n",
        "            if \"masks\" in target:\n",
        "                target[\"masks\"] = F.resize(\n",
        "                    target[\"masks\"],\n",
        "                    [new_height, new_width],\n",
        "                    interpolation=InterpolationMode.NEAREST,\n",
        "                    antialias=self.antialias,\n",
        "                )\n",
        "\n",
        "        return image, target\n",
        "\n",
        "\n",
        "class FixedSizeCrop(nn.Module):\n",
        "    def __init__(self, size, fill=0, padding_mode=\"constant\"):\n",
        "        super().__init__()\n",
        "        size = tuple(T._setup_size(size, error_msg=\"Please provide only two dimensions (h, w) for size.\"))\n",
        "        self.crop_height = size[0]\n",
        "        self.crop_width = size[1]\n",
        "        self.fill = fill  # TODO: Fill is currently respected only on PIL. Apply tensor patch.\n",
        "        self.padding_mode = padding_mode\n",
        "\n",
        "    def _pad(self, img, target, padding):\n",
        "        # Taken from the functional_tensor.py pad\n",
        "        if isinstance(padding, int):\n",
        "            pad_left = pad_right = pad_top = pad_bottom = padding\n",
        "        elif len(padding) == 1:\n",
        "            pad_left = pad_right = pad_top = pad_bottom = padding[0]\n",
        "        elif len(padding) == 2:\n",
        "            pad_left = pad_right = padding[0]\n",
        "            pad_top = pad_bottom = padding[1]\n",
        "        else:\n",
        "            pad_left = padding[0]\n",
        "            pad_top = padding[1]\n",
        "            pad_right = padding[2]\n",
        "            pad_bottom = padding[3]\n",
        "\n",
        "        padding = [pad_left, pad_top, pad_right, pad_bottom]\n",
        "        img = F.pad(img, padding, self.fill, self.padding_mode)\n",
        "        if target is not None:\n",
        "            target[\"boxes\"][:, 0::2] += pad_left\n",
        "            target[\"boxes\"][:, 1::2] += pad_top\n",
        "            if \"masks\" in target:\n",
        "                target[\"masks\"] = F.pad(target[\"masks\"], padding, 0, \"constant\")\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def _crop(self, img, target, top, left, height, width):\n",
        "        img = F.crop(img, top, left, height, width)\n",
        "        if target is not None:\n",
        "            boxes = target[\"boxes\"]\n",
        "            boxes[:, 0::2] -= left\n",
        "            boxes[:, 1::2] -= top\n",
        "            boxes[:, 0::2].clamp_(min=0, max=width)\n",
        "            boxes[:, 1::2].clamp_(min=0, max=height)\n",
        "\n",
        "            is_valid = (boxes[:, 0] < boxes[:, 2]) & (boxes[:, 1] < boxes[:, 3])\n",
        "\n",
        "            target[\"boxes\"] = boxes[is_valid]\n",
        "            target[\"labels\"] = target[\"labels\"][is_valid]\n",
        "            if \"masks\" in target:\n",
        "                target[\"masks\"] = F.crop(target[\"masks\"][is_valid], top, left, height, width)\n",
        "\n",
        "        return img, target\n",
        "\n",
        "    def forward(self, img, target=None):\n",
        "        _, height, width = F.get_dimensions(img)\n",
        "        new_height = min(height, self.crop_height)\n",
        "        new_width = min(width, self.crop_width)\n",
        "\n",
        "        if new_height != height or new_width != width:\n",
        "            offset_height = max(height - self.crop_height, 0)\n",
        "            offset_width = max(width - self.crop_width, 0)\n",
        "\n",
        "            r = torch.rand(1)\n",
        "            top = int(offset_height * r)\n",
        "            left = int(offset_width * r)\n",
        "\n",
        "            img, target = self._crop(img, target, top, left, new_height, new_width)\n",
        "\n",
        "        pad_bottom = max(self.crop_height - new_height, 0)\n",
        "        pad_right = max(self.crop_width - new_width, 0)\n",
        "        if pad_bottom != 0 or pad_right != 0:\n",
        "            img, target = self._pad(img, target, [0, 0, pad_right, pad_bottom])\n",
        "\n",
        "        return img, target\n",
        "\n",
        "\n",
        "class RandomShortestSize(nn.Module):\n",
        "    def __init__(\n",
        "        self,\n",
        "        min_size: Union[List[int], Tuple[int], int],\n",
        "        max_size: int,\n",
        "        interpolation: InterpolationMode = InterpolationMode.BILINEAR,\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.min_size = [min_size] if isinstance(min_size, int) else list(min_size)\n",
        "        self.max_size = max_size\n",
        "        self.interpolation = interpolation\n",
        "\n",
        "    def forward(\n",
        "        self, image: Tensor, target: Optional[Dict[str, Tensor]] = None\n",
        "    ) -> Tuple[Tensor, Optional[Dict[str, Tensor]]]:\n",
        "        _, orig_height, orig_width = F.get_dimensions(image)\n",
        "\n",
        "        min_size = self.min_size[torch.randint(len(self.min_size), (1,)).item()]\n",
        "        r = min(min_size / min(orig_height, orig_width), self.max_size / max(orig_height, orig_width))\n",
        "\n",
        "        new_width = int(orig_width * r)\n",
        "        new_height = int(orig_height * r)\n",
        "\n",
        "        image = F.resize(image, [new_height, new_width], interpolation=self.interpolation)\n",
        "\n",
        "        if target is not None:\n",
        "            target[\"boxes\"][:, 0::2] *= new_width / orig_width\n",
        "            target[\"boxes\"][:, 1::2] *= new_height / orig_height\n",
        "            if \"masks\" in target:\n",
        "                target[\"masks\"] = F.resize(\n",
        "                    target[\"masks\"], [new_height, new_width], interpolation=InterpolationMode.NEAREST\n",
        "                )\n",
        "\n",
        "        return image, target\n",
        "\n",
        "\n",
        "def _copy_paste(\n",
        "    image: torch.Tensor,\n",
        "    target: Dict[str, Tensor],\n",
        "    paste_image: torch.Tensor,\n",
        "    paste_target: Dict[str, Tensor],\n",
        "    blending: bool = True,\n",
        "    resize_interpolation: F.InterpolationMode = F.InterpolationMode.BILINEAR,\n",
        ") -> Tuple[torch.Tensor, Dict[str, Tensor]]:\n",
        "\n",
        "    # Random paste targets selection:\n",
        "    num_masks = len(paste_target[\"masks\"])\n",
        "\n",
        "    if num_masks < 1:\n",
        "        # Such degerante case with num_masks=0 can happen with LSJ\n",
        "        # Let's just return (image, target)\n",
        "        return image, target\n",
        "\n",
        "    # We have to please torch script by explicitly specifying dtype as torch.long\n",
        "    random_selection = torch.randint(0, num_masks, (num_masks,), device=paste_image.device)\n",
        "    random_selection = torch.unique(random_selection).to(torch.long)\n",
        "\n",
        "    paste_masks = paste_target[\"masks\"][random_selection]\n",
        "    paste_boxes = paste_target[\"boxes\"][random_selection]\n",
        "    paste_labels = paste_target[\"labels\"][random_selection]\n",
        "\n",
        "    masks = target[\"masks\"]\n",
        "\n",
        "    # We resize source and paste data if they have different sizes\n",
        "    # This is something we introduced here as originally the algorithm works\n",
        "    # on equal-sized data (for example, coming from LSJ data augmentations)\n",
        "    size1 = image.shape[-2:]\n",
        "    size2 = paste_image.shape[-2:]\n",
        "    if size1 != size2:\n",
        "        paste_image = F.resize(paste_image, size1, interpolation=resize_interpolation)\n",
        "        paste_masks = F.resize(paste_masks, size1, interpolation=F.InterpolationMode.NEAREST)\n",
        "        # resize bboxes:\n",
        "        ratios = torch.tensor((size1[1] / size2[1], size1[0] / size2[0]), device=paste_boxes.device)\n",
        "        paste_boxes = paste_boxes.view(-1, 2, 2).mul(ratios).view(paste_boxes.shape)\n",
        "\n",
        "    paste_alpha_mask = paste_masks.sum(dim=0) > 0\n",
        "\n",
        "    if blending:\n",
        "        paste_alpha_mask = F.gaussian_blur(\n",
        "            paste_alpha_mask.unsqueeze(0),\n",
        "            kernel_size=(5, 5),\n",
        "            sigma=[\n",
        "                2.0,\n",
        "            ],\n",
        "        )\n",
        "\n",
        "    # Copy-paste images:\n",
        "    image = (image * (~paste_alpha_mask)) + (paste_image * paste_alpha_mask)\n",
        "\n",
        "    # Copy-paste masks:\n",
        "    masks = masks * (~paste_alpha_mask)\n",
        "    non_all_zero_masks = masks.sum((-1, -2)) > 0\n",
        "    masks = masks[non_all_zero_masks]\n",
        "\n",
        "    # Do a shallow copy of the target dict\n",
        "    out_target = {k: v for k, v in target.items()}\n",
        "\n",
        "    out_target[\"masks\"] = torch.cat([masks, paste_masks])\n",
        "\n",
        "    # Copy-paste boxes and labels\n",
        "    boxes = ops.masks_to_boxes(masks)\n",
        "    out_target[\"boxes\"] = torch.cat([boxes, paste_boxes])\n",
        "\n",
        "    labels = target[\"labels\"][non_all_zero_masks]\n",
        "    out_target[\"labels\"] = torch.cat([labels, paste_labels])\n",
        "\n",
        "    # Update additional optional keys: area and iscrowd if exist\n",
        "    if \"area\" in target:\n",
        "        out_target[\"area\"] = out_target[\"masks\"].sum((-1, -2)).to(torch.float32)\n",
        "\n",
        "    if \"iscrowd\" in target and \"iscrowd\" in paste_target:\n",
        "        # target['iscrowd'] size can be differ from mask size (non_all_zero_masks)\n",
        "        # For example, if previous transforms geometrically modifies masks/boxes/labels but\n",
        "        # does not update \"iscrowd\"\n",
        "        if len(target[\"iscrowd\"]) == len(non_all_zero_masks):\n",
        "            iscrowd = target[\"iscrowd\"][non_all_zero_masks]\n",
        "            paste_iscrowd = paste_target[\"iscrowd\"][random_selection]\n",
        "            out_target[\"iscrowd\"] = torch.cat([iscrowd, paste_iscrowd])\n",
        "\n",
        "    # Check for degenerated boxes and remove them\n",
        "    boxes = out_target[\"boxes\"]\n",
        "    degenerate_boxes = boxes[:, 2:] <= boxes[:, :2]\n",
        "    if degenerate_boxes.any():\n",
        "        valid_targets = ~degenerate_boxes.any(dim=1)\n",
        "\n",
        "        out_target[\"boxes\"] = boxes[valid_targets]\n",
        "        out_target[\"masks\"] = out_target[\"masks\"][valid_targets]\n",
        "        out_target[\"labels\"] = out_target[\"labels\"][valid_targets]\n",
        "\n",
        "        if \"area\" in out_target:\n",
        "            out_target[\"area\"] = out_target[\"area\"][valid_targets]\n",
        "        if \"iscrowd\" in out_target and len(out_target[\"iscrowd\"]) == len(valid_targets):\n",
        "            out_target[\"iscrowd\"] = out_target[\"iscrowd\"][valid_targets]\n",
        "\n",
        "    return image, out_target\n",
        "\n",
        "\n",
        "class SimpleCopyPaste(torch.nn.Module):\n",
        "    def __init__(self, blending=True, resize_interpolation=F.InterpolationMode.BILINEAR):\n",
        "        super().__init__()\n",
        "        self.resize_interpolation = resize_interpolation\n",
        "        self.blending = blending\n",
        "\n",
        "    def forward(\n",
        "        self, images: List[torch.Tensor], targets: List[Dict[str, Tensor]]\n",
        "    ) -> Tuple[List[torch.Tensor], List[Dict[str, Tensor]]]:\n",
        "        torch._assert(\n",
        "            isinstance(images, (list, tuple)) and all([isinstance(v, torch.Tensor) for v in images]),\n",
        "            \"images should be a list of tensors\",\n",
        "        )\n",
        "        torch._assert(\n",
        "            isinstance(targets, (list, tuple)) and len(images) == len(targets),\n",
        "            \"targets should be a list of the same size as images\",\n",
        "        )\n",
        "        for target in targets:\n",
        "            # Can not check for instance type dict with inside torch.jit.script\n",
        "            # torch._assert(isinstance(target, dict), \"targets item should be a dict\")\n",
        "            for k in [\"masks\", \"boxes\", \"labels\"]:\n",
        "                torch._assert(k in target, f\"Key {k} should be present in targets\")\n",
        "                torch._assert(isinstance(target[k], torch.Tensor), f\"Value for the key {k} should be a tensor\")\n",
        "\n",
        "        # images = [t1, t2, ..., tN]\n",
        "        # Let's define paste_images as shifted list of input images\n",
        "        # paste_images = [t2, t3, ..., tN, t1]\n",
        "        # FYI: in TF they mix data on the dataset level\n",
        "        images_rolled = images[-1:] + images[:-1]\n",
        "        targets_rolled = targets[-1:] + targets[:-1]\n",
        "\n",
        "        output_images: List[torch.Tensor] = []\n",
        "        output_targets: List[Dict[str, Tensor]] = []\n",
        "\n",
        "        for image, target, paste_image, paste_target in zip(images, targets, images_rolled, targets_rolled):\n",
        "            output_image, output_data = _copy_paste(\n",
        "                image,\n",
        "                target,\n",
        "                paste_image,\n",
        "                paste_target,\n",
        "                blending=self.blending,\n",
        "                resize_interpolation=self.resize_interpolation,\n",
        "            )\n",
        "            output_images.append(output_image)\n",
        "            output_targets.append(output_data)\n",
        "\n",
        "        return output_images, output_targets\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        s = f\"{self.__class__.__name__}(blending={self.blending}, resize_interpolation={self.resize_interpolation})\"\n",
        "        return s"
      ],
      "metadata": {
        "id": "odDXGch7xtzJ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms import v2 as T\n",
        "\n",
        "\n",
        "def get_transform(train):\n",
        "    transforms = []\n",
        "    if train:\n",
        "        transforms.append(T.RandomHorizontalFlip(0.5))\n",
        "    transforms.append(T.ToDtype(torch.float, scale=True))\n",
        "    transforms.append(T.ToPureTensor())\n",
        "    return T.Compose(transforms)"
      ],
      "metadata": {
        "id": "LGo3b1QuuJ1f"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import utils\n",
        "\n",
        "\n",
        "model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights=\"DEFAULT\")\n",
        "dataset = PennFudanDataset('data/PennFudanPed', get_transform(train=True))\n",
        "data_loader = torch.utils.data.DataLoader(\n",
        "    dataset,\n",
        "    batch_size=2,\n",
        "    shuffle=True,\n",
        "    num_workers=4,\n",
        "    collate_fn=utils.collate_fn\n",
        ")\n",
        "\n",
        "# For Training\n",
        "images, targets = next(iter(data_loader))\n",
        "images = list(image for image in images)\n",
        "targets = [{k: v for k, v in t.items()} for t in targets]\n",
        "output = model(images, targets)  # Returns losses and detections\n",
        "print(output)\n",
        "\n",
        "# For inference\n",
        "model.eval()\n",
        "x = [torch.rand(3, 300, 400), torch.rand(3, 500, 400)]\n",
        "predictions = model(x)  # Returns predictions\n",
        "print(predictions[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "CMQ6PbDOuJ4X",
        "outputId": "d96e5c4b-dab3-4fb0-beaa-85e1b444725a"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-2dfb92c3e36f>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchvision\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfasterrcnn_resnet50_fpn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"DEFAULT\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPennFudanDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/PennFudanPed'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m data_loader = torch.utils.data.DataLoader(\n\u001b[1;32m      7\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-85a2e28588eb>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, root, transforms)\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;31m# load all image files, sorting them to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# ensure that they are aligned\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"PNGImages\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"PedMasks\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/PennFudanPed/PNGImages'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3Sw8J7eWxEiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "So6_vno6xEnM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GyEXD63sxEvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WhGmzpB2xEyo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
